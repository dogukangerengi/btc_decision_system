# =============================================================================
# VERÄ° Ã–N Ä°ÅLEME MODÃœLÃœ (DATA PREPROCESSOR)
# =============================================================================
# AmaÃ§: Ham OHLCV verisini analiz iÃ§in hazÄ±rlamak
# - Eksik veri tespiti ve doldurma (imputation)
# - Outlier tespiti ve iÅŸleme
# - Return hesaplama
# - Veri normalizasyonu
# 
# Ä°statistiksel Not: Veri Ã¶n iÅŸleme, tÃ¼m downstream analizlerin kalitesini
# doÄŸrudan etkiler. Ã–zellikle look-ahead bias'a dikkat edilmelidir.
# =============================================================================

import pandas as pd                      # Veri manipÃ¼lasyonu
import numpy as np                       # SayÄ±sal hesaplamalar
from typing import Optional, Tuple, List, Dict  # Tip belirteÃ§leri
from scipy import stats                  # Ä°statistiksel fonksiyonlar


class DataPreprocessor:
    """
    OHLCV verisini Ã¶n iÅŸleme tabi tutan sÄ±nÄ±f.
    
    Ä°statistiksel Ã–nem:
    ------------------
    1. Missing data handling: Bias Ã¶nleme iÃ§in kritik
    2. Outlier treatment: Robust istatistikler iÃ§in gerekli
    3. Return calculation: Log vs simple return seÃ§imi Ã¶nemli
    4. Stationarity: Ã‡oÄŸu model duraÄŸan seri gerektirir
    """
    
    def __init__(self):
        """
        Preprocessor sÄ±nÄ±fÄ±nÄ± baÅŸlatÄ±r.
        Stateless tasarÄ±m: Her method baÄŸÄ±msÄ±z Ã§alÄ±ÅŸÄ±r.
        """
        pass
    
    # =========================================================================
    # EKSÄ°K VERÄ° Ä°ÅLEME
    # =========================================================================
    
    def handle_missing_values(
        self,
        df: pd.DataFrame,
        method: str = "ffill",            # Doldurma yÃ¶ntemi
        max_gap: int = 5                  # Maksimum ardÄ±ÅŸÄ±k eksik veri
    ) -> pd.DataFrame:
        """
        Eksik verileri tespit eder ve doldurur.
        
        Parametreler:
        ------------
        df : pd.DataFrame
            OHLCV DataFrame
            
        method : str
            Doldurma yÃ¶ntemi:
            - "ffill": Forward fill (Ã¶nceki deÄŸerle doldur)
            - "bfill": Backward fill (sonraki deÄŸerle doldur)
            - "interpolate": Linear interpolasyon
            - "drop": Eksik satÄ±rlarÄ± sil
            
        max_gap : int
            ArdÄ±ÅŸÄ±k eksik veri sayÄ±sÄ± bu deÄŸeri aÅŸarsa doldurma yapÄ±lmaz
            (Ã‡ok uzun gap'ler genellikle market closure'Ä± gÃ¶sterir)
        
        DÃ¶ndÃ¼rÃ¼r:
        --------
        pd.DataFrame
            Eksik deÄŸerleri iÅŸlenmiÅŸ DataFrame
        
        Ä°statistiksel Not:
        -----------------
        - Forward fill: Look-ahead bias riski YOK (Ã¶nceki veri kullanÄ±lÄ±r)
        - Backward fill: Look-ahead bias riski VAR (gelecek veri kullanÄ±lÄ±r)
        - Interpolation: KÄ±smi look-ahead bias riski
        
        Backtest iÃ§in SADECE forward fill Ã¶nerilir.
        """
        
        df_clean = df.copy()              # Orijinal veriyi koru
        
        # Eksik deÄŸer istatistikleri
        missing_before = df_clean.isnull().sum().sum()
        
        if missing_before == 0:
            print("âœ“ Eksik deÄŸer bulunamadÄ±")
            return df_clean
        
        print(f"âš  {missing_before} eksik deÄŸer tespit edildi")
        
        # ArdÄ±ÅŸÄ±k eksik deÄŸer kontrolÃ¼
        # max_gap'ten uzun boÅŸluklarÄ± iÅŸaretle
        for col in df_clean.columns:
            # ArdÄ±ÅŸÄ±k NaN gruplarÄ±nÄ± bul
            mask = df_clean[col].isnull()
            # Grup numaralarÄ±nÄ± ata
            groups = (mask != mask.shift()).cumsum()
            # Her grubun uzunluÄŸunu hesapla
            group_sizes = mask.groupby(groups).transform('sum')
            # Ã‡ok uzun gap'leri iÅŸaretle (doldurmayacaÄŸÄ±z)
            long_gaps = (group_sizes > max_gap) & mask
            
            if long_gaps.any():
                print(f"  âš  {col}: {long_gaps.sum()} deÄŸer {max_gap}+ uzunluÄŸunda gap iÃ§inde (doldurulmayacak)")
        
        # Doldurma yÃ¶ntemi uygula
        if method == "ffill":
            # Forward fill: Ã–nceki geÃ§erli deÄŸerle doldur
            # limit parametresi max_gap kadar doldurmayÄ± sÄ±nÄ±rlar
            df_clean = df_clean.ffill(limit=max_gap)
            
        elif method == "bfill":
            # Backward fill: Sonraki geÃ§erli deÄŸerle doldur
            # DÄ°KKAT: Look-ahead bias riski!
            print("  âš  UYARI: bfill look-ahead bias riski taÅŸÄ±r!")
            df_clean = df_clean.bfill(limit=max_gap)
            
        elif method == "interpolate":
            # Linear interpolasyon
            # DÄ°KKAT: KÄ±smi look-ahead bias riski
            print("  âš  UYARI: interpolate kÄ±smi look-ahead bias riski taÅŸÄ±r!")
            df_clean = df_clean.interpolate(method='linear', limit=max_gap)
            
        elif method == "drop":
            # Eksik satÄ±rlarÄ± tamamen sil
            df_clean = df_clean.dropna()
            
        else:
            raise ValueError(f"GeÃ§ersiz method: {method}")
        
        # SonuÃ§ istatistikleri
        missing_after = df_clean.isnull().sum().sum()
        print(f"âœ“ {missing_before - missing_after} eksik deÄŸer dolduruldu")
        print(f"  Kalan eksik: {missing_after}")
        
        return df_clean
    
    # =========================================================================
    # RETURN HESAPLAMA
    # =========================================================================
    
    def calculate_returns(
        self,
        df: pd.DataFrame,
        method: str = "log",              # Return hesaplama yÃ¶ntemi
        periods: int = 1                  # KaÃ§ periyot sonrasÄ± return
    ) -> pd.DataFrame:
        """
        Fiyat verisinden return (getiri) hesaplar.
        
        Parametreler:
        ------------
        df : pd.DataFrame
            OHLCV DataFrame (en az 'close' kolonu olmalÄ±)
            
        method : str
            Return hesaplama yÃ¶ntemi:
            - "log": Logaritmik return (ln(P_t / P_{t-1}))
            - "simple": Basit return ((P_t - P_{t-1}) / P_{t-1})
            
        periods : int
            Forward return periyodu
            1 = bir sonraki bar'Ä±n getirisi
            
        DÃ¶ndÃ¼rÃ¼r:
        --------
        pd.DataFrame
            Orijinal kolonlar + 'returns' kolonu
        
        Ä°statistiksel Not:
        -----------------
        Log return avantajlarÄ±:
        1. Toplamsal: r_total = r_1 + r_2 + ... + r_n
        2. Negatif simetri: -100% ile sÄ±nÄ±rlÄ± deÄŸil
        3. Normal daÄŸÄ±lÄ±ma daha yakÄ±n (genellikle)
        4. Volatilite hesaplamalarÄ± iÃ§in daha uygun
        
        Simple return avantajlarÄ±:
        1. YorumlamasÄ± kolay (%5 return = %5 kazanÃ§)
        2. PortfÃ¶y return'Ã¼ doÄŸrudan hesaplanabilir
        
        Backtest iÃ§in genellikle log return tercih edilir.
        """
        
        df_with_returns = df.copy()
        
        if method == "log":
            # Log return: ln(P_t) - ln(P_{t-1}) = ln(P_t / P_{t-1})
            # np.log kullanÄ±yoruz (doÄŸal logaritma)
            df_with_returns['returns'] = np.log(
                df_with_returns['close'] / df_with_returns['close'].shift(periods)
            )
            
        elif method == "simple":
            # Simple return: (P_t - P_{t-1}) / P_{t-1}
            # pct_change() bunu otomatik hesaplar
            df_with_returns['returns'] = df_with_returns['close'].pct_change(periods)
            
        else:
            raise ValueError(f"GeÃ§ersiz method: {method}. 'log' veya 'simple' kullanÄ±n.")
        
        # Forward return (gelecek getiri - sinyal deÄŸerlendirmesi iÃ§in)
        # DÄ°KKAT: Bu kolon backtest'te kullanÄ±lmalÄ±, look-ahead bias'a dikkat!
        df_with_returns['forward_returns'] = df_with_returns['returns'].shift(-periods)
        
        return df_with_returns
    
    # =========================================================================
    # OUTLIER TESPÄ°TÄ° VE Ä°ÅLEME
    # =========================================================================
    
    def detect_outliers(
        self,
        df: pd.DataFrame,
        column: str = "returns",          # Outlier tespit edilecek kolon
        method: str = "zscore",           # Tespit yÃ¶ntemi
        threshold: float = 3.0            # EÅŸik deÄŸer
    ) -> pd.Series:
        """
        Outlier'larÄ± (aykÄ±rÄ± deÄŸerler) tespit eder.
        
        Parametreler:
        ------------
        df : pd.DataFrame
            Veri DataFrame'i
            
        column : str
            Outlier tespiti yapÄ±lacak kolon
            
        method : str
            Tespit yÃ¶ntemi:
            - "zscore": Z-skor (standart sapma bazlÄ±)
            - "iqr": Interquartile Range (Q1-Q3 bazlÄ±)
            - "mad": Median Absolute Deviation (robust)
            
        threshold : float
            Outlier eÅŸiÄŸi
            - zscore iÃ§in: genellikle 3.0 (3 sigma kuralÄ±)
            - iqr iÃ§in: genellikle 1.5 (Tukey's fence)
            - mad iÃ§in: genellikle 3.5
        
        DÃ¶ndÃ¼rÃ¼r:
        --------
        pd.Series
            Boolean mask (True = outlier)
        
        Ä°statistiksel Not:
        -----------------
        - Z-score: Normal daÄŸÄ±lÄ±m varsayar, outlier'lara duyarlÄ±
        - IQR: DaÄŸÄ±lÄ±m agnostik, orta derecede robust
        - MAD: En robust yÃ¶ntem, fat-tailed daÄŸÄ±lÄ±mlar iÃ§in ideal
        
        Finansal veri genellikle fat-tailed olduÄŸu iÃ§in MAD Ã¶nerilir.
        """
        
        data = df[column].dropna()        # NaN'larÄ± kaldÄ±r
        
        if method == "zscore":
            # Z-score: (x - mean) / std
            # |z| > threshold ise outlier
            z_scores = np.abs(stats.zscore(data))
            outlier_mask = pd.Series(z_scores > threshold, index=data.index)
            
        elif method == "iqr":
            # IQR yÃ¶ntemi: Q1 - threshold*IQR ile Q3 + threshold*IQR dÄ±ÅŸÄ±
            Q1 = data.quantile(0.25)      # 1. Ã§eyreklik (25. percentile)
            Q3 = data.quantile(0.75)      # 3. Ã§eyreklik (75. percentile)
            IQR = Q3 - Q1                 # Interquartile range
            
            lower_bound = Q1 - threshold * IQR
            upper_bound = Q3 + threshold * IQR
            
            outlier_mask = (data < lower_bound) | (data > upper_bound)
            
        elif method == "mad":
            # MAD: Median Absolute Deviation
            # Robust alternatif: median bazlÄ±, outlier'lara dayanÄ±klÄ±
            median = data.median()
            # Her noktanÄ±n mediandan farkÄ±nÄ±n mutlak deÄŸeri
            mad = np.median(np.abs(data - median))
            # MAD'Ä± standart sapma Ã¶lÃ§eÄŸine Ã§evirmek iÃ§in sabit
            # Normal daÄŸÄ±lÄ±m iÃ§in: MAD * 1.4826 â‰ˆ std
            mad_scaled = mad * 1.4826
            
            # Modified z-score
            modified_z = np.abs((data - median) / mad_scaled)
            outlier_mask = pd.Series(modified_z > threshold, index=data.index)
            
        else:
            raise ValueError(f"GeÃ§ersiz method: {method}")
        
        # Tam index'e geniÅŸlet (NaN'lar False olarak)
        full_mask = pd.Series(False, index=df.index)
        full_mask[outlier_mask.index] = outlier_mask
        
        print(f"ğŸ“Š Outlier tespiti ({method}, threshold={threshold}):")
        print(f"   Toplam outlier: {full_mask.sum()} / {len(df)} ({100*full_mask.mean():.2f}%)")
        
        return full_mask
    
    def handle_outliers(
        self,
        df: pd.DataFrame,
        column: str = "returns",
        method: str = "winsorize",        # Ä°ÅŸleme yÃ¶ntemi
        limits: Tuple[float, float] = (0.01, 0.01)  # Winsorize limitleri
    ) -> pd.DataFrame:
        """
        Outlier'larÄ± iÅŸler (temizler veya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r).
        
        Parametreler:
        ------------
        df : pd.DataFrame
            Veri DataFrame'i
            
        column : str
            Ä°ÅŸlenecek kolon
            
        method : str
            Ä°ÅŸleme yÃ¶ntemi:
            - "winsorize": UÃ§ deÄŸerleri percentile deÄŸerlerine Ã§ek
            - "clip": Belirli min-max aralÄ±ÄŸÄ±na sÄ±nÄ±rla
            - "remove": Outlier satÄ±rlarÄ± sil
            - "nan": Outlier'larÄ± NaN yap
            
        limits : Tuple[float, float]
            Winsorize iÃ§in alt ve Ã¼st percentile limitleri
            (0.01, 0.01) = %1 alt ve Ã¼st uÃ§lar
        
        DÃ¶ndÃ¼rÃ¼r:
        --------
        pd.DataFrame
            Outlier'larÄ± iÅŸlenmiÅŸ DataFrame
        
        Ä°statistiksel Not:
        -----------------
        Winsorization tercih edilir Ã§Ã¼nkÃ¼:
        1. Veri kaybÄ± yok (remove'un aksine)
        2. UÃ§ deÄŸerlerin etkisi azaltÄ±lÄ±r
        3. DaÄŸÄ±lÄ±mÄ±n genel yapÄ±sÄ± korunur
        
        Dikkat: GerÃ§ek piyasa crash'leri de "outlier" gÃ¶rÃ¼nebilir.
        BunlarÄ± tamamen silmek unrealistic backtest'e yol aÃ§ar.
        """
        
        df_processed = df.copy()
        
        if method == "winsorize":
            # Winsorization: UÃ§ deÄŸerleri percentile deÄŸerlerine Ã§ek
            # scipy.stats.mstats.winsorize kullanÄ±labilir ama manuel daha kontrollÃ¼
            lower_percentile = limits[0]
            upper_percentile = 1 - limits[1]
            
            lower_val = df_processed[column].quantile(lower_percentile)
            upper_val = df_processed[column].quantile(upper_percentile)
            
            # Clip: lower_val ile upper_val arasÄ±na sÄ±nÄ±rla
            df_processed[column] = df_processed[column].clip(lower_val, upper_val)
            
            print(f"âœ“ Winsorization uygulandÄ±: [{lower_val:.4f}, {upper_val:.4f}]")
            
        elif method == "clip":
            # Manuel clip: Sabit deÄŸerlerle sÄ±nÄ±rla
            lower_val, upper_val = limits
            df_processed[column] = df_processed[column].clip(lower_val, upper_val)
            
        elif method == "remove":
            # Outlier'larÄ± tespit et ve sil
            outlier_mask = self.detect_outliers(df, column)
            df_processed = df_processed[~outlier_mask]
            print(f"âœ“ {outlier_mask.sum()} outlier satÄ±r silindi")
            
        elif method == "nan":
            # Outlier'larÄ± NaN yap
            outlier_mask = self.detect_outliers(df, column)
            df_processed.loc[outlier_mask, column] = np.nan
            print(f"âœ“ {outlier_mask.sum()} outlier NaN yapÄ±ldÄ±")
            
        else:
            raise ValueError(f"GeÃ§ersiz method: {method}")
        
        return df_processed
    
    # =========================================================================
    # VOLATÄ°LÄ°TE HESAPLAMA
    # =========================================================================
    
    def calculate_volatility(
        self,
        df: pd.DataFrame,
        window: int = 20,                 # Rolling window boyutu
        method: str = "standard",         # Volatilite hesaplama yÃ¶ntemi
        annualize: bool = True,           # YÄ±llÄ±klaÅŸtÄ±r
        periods_per_year: int = 252 * 24  # Saatlik veri iÃ§in
    ) -> pd.DataFrame:
        """
        Rolling volatilite hesaplar.
        
        Parametreler:
        ------------
        df : pd.DataFrame
            Returns kolonu iÃ§eren DataFrame
            
        window : int
            Rolling window boyutu (bar sayÄ±sÄ±)
            
        method : str
            Volatilite hesaplama yÃ¶ntemi:
            - "standard": Standart sapma
            - "parkinson": High-Low bazlÄ± (daha verimli)
            - "garman_klass": OHLC bazlÄ± (en verimli)
            
        annualize : bool
            True ise yÄ±llÄ±klaÅŸtÄ±rÄ±lmÄ±ÅŸ volatilite dÃ¶ndÃ¼r
            
        periods_per_year : int
            Bir yÄ±ldaki periyot sayÄ±sÄ±
            1h veri iÃ§in: 252 * 24 = 6048
            1d veri iÃ§in: 252
        
        DÃ¶ndÃ¼rÃ¼r:
        --------
        pd.DataFrame
            Orijinal kolonlar + 'volatility' kolonu
        
        Ä°statistiksel Not:
        -----------------
        Parkinson ve Garman-Klass volatility estimator'larÄ±
        sadece close fiyatÄ±na dayanan standart sapmadan daha
        verimlidir (efficient). OHLC bilgisini kullanÄ±rlar.
        
        VerimliliÄŸi karÅŸÄ±laÅŸtÄ±rma:
        - Standard: baseline
        - Parkinson: ~5x daha efficient
        - Garman-Klass: ~8x daha efficient
        """
        
        df_vol = df.copy()
        
        if method == "standard":
            # Standart sapma: sqrt(variance of returns)
            # Rolling pencere Ã¼zerinde hesapla
            rolling_vol = df_vol['returns'].rolling(window=window).std()
            
        elif method == "parkinson":
            # Parkinson volatility: High-Low range bazlÄ±
            # FormÃ¼l: sqrt(1/(4*ln(2)) * (ln(H/L))^2)
            # Daha verimli Ã§Ã¼nkÃ¼ intrabar bilgi kullanÄ±r
            log_hl = np.log(df_vol['high'] / df_vol['low'])
            parkinson_factor = 1 / (4 * np.log(2))
            rolling_vol = np.sqrt(
                parkinson_factor * (log_hl ** 2).rolling(window=window).mean()
            )
            
        elif method == "garman_klass":
            # Garman-Klass volatility: OHLC bazlÄ±
            # En verimli estimator (drift-adjusted)
            log_hl = np.log(df_vol['high'] / df_vol['low'])
            log_co = np.log(df_vol['close'] / df_vol['open'])
            
            # Garman-Klass formÃ¼lÃ¼
            gk = 0.5 * (log_hl ** 2) - (2 * np.log(2) - 1) * (log_co ** 2)
            rolling_vol = np.sqrt(gk.rolling(window=window).mean())
            
        else:
            raise ValueError(f"GeÃ§ersiz method: {method}")
        
        # YÄ±llÄ±klaÅŸtÄ±rma
        if annualize:
            # Volatilite sqrt(T) ile Ã¶lÃ§eklenir
            rolling_vol = rolling_vol * np.sqrt(periods_per_year)
        
        df_vol['volatility'] = rolling_vol
        
        return df_vol
    
    # =========================================================================
    # PIPELINE: TÃœM Ã–N Ä°ÅLEME ADIMLARI
    # =========================================================================
    
    def full_pipeline(
        self,
        df: pd.DataFrame,
        config: Optional[Dict] = None
    ) -> pd.DataFrame:
        """
        TÃ¼m Ã¶n iÅŸleme adÄ±mlarÄ±nÄ± sÄ±rasÄ±yla uygular.
        
        Parametreler:
        ------------
        df : pd.DataFrame
            Ham OHLCV DataFrame
            
        config : Dict, optional
            Ã–zel yapÄ±landÄ±rma. VarsayÄ±lan deÄŸerler kullanÄ±lÄ±r.
        
        DÃ¶ndÃ¼rÃ¼r:
        --------
        pd.DataFrame
            Tam iÅŸlenmiÅŸ, analize hazÄ±r DataFrame
        
        Pipeline AdÄ±mlarÄ±:
        -----------------
        1. Missing value handling (ffill)
        2. Return calculation (log returns)
        3. Outlier winsorization
        4. Volatility calculation (Garman-Klass)
        5. Forward return ekleme (sinyal deÄŸerlendirmesi iÃ§in)
        """
        
        # VarsayÄ±lan konfigÃ¼rasyon
        default_config = {
            'missing_method': 'ffill',
            'missing_max_gap': 5,
            'return_method': 'log',
            'return_periods': 1,
            'outlier_method': 'winsorize',
            'outlier_limits': (0.01, 0.01),
            'volatility_window': 20,
            'volatility_method': 'garman_klass',
        }
        
        # Config gÃ¼ncelle
        if config:
            default_config.update(config)
        cfg = default_config
        
        print("=" * 50)
        print("VERÄ° Ã–N Ä°ÅLEME PIPELINE")
        print("=" * 50)
        
        # AdÄ±m 1: Missing values
        print("\n[1/4] Missing value iÅŸleme...")
        df_processed = self.handle_missing_values(
            df,
            method=cfg['missing_method'],
            max_gap=cfg['missing_max_gap']
        )
        
        # AdÄ±m 2: Returns
        print("\n[2/4] Return hesaplama...")
        df_processed = self.calculate_returns(
            df_processed,
            method=cfg['return_method'],
            periods=cfg['return_periods']
        )
        
        # AdÄ±m 3: Outliers
        print("\n[3/4] Outlier iÅŸleme...")
        df_processed = self.handle_outliers(
            df_processed,
            column='returns',
            method=cfg['outlier_method'],
            limits=cfg['outlier_limits']
        )
        
        # AdÄ±m 4: Volatility
        print("\n[4/4] Volatilite hesaplama...")
        df_processed = self.calculate_volatility(
            df_processed,
            window=cfg['volatility_window'],
            method=cfg['volatility_method']
        )
        
        # Ä°lk birkaÃ§ NaN satÄ±rÄ± kaldÄ±r (rolling hesaplamalardan)
        df_processed = df_processed.dropna()
        
        print("\n" + "=" * 50)
        print(f"âœ“ Pipeline tamamlandÄ±: {len(df_processed)} satÄ±r hazÄ±r")
        print("=" * 50)
        
        return df_processed


# =============================================================================
# MODÃœL TEST KODU
# =============================================================================

if __name__ == "__main__":
    
    # Test iÃ§in Ã¶rnek veri oluÅŸtur
    print("=" * 60)
    print("DATA PREPROCESSOR TEST")
    print("=" * 60)
    
    # Ã–nce gerÃ§ek veri Ã§ekelim (fetcher modÃ¼lÃ¼nden)
    from fetcher import DataFetcher
    
    # Veri Ã§ek
    fetcher = DataFetcher(exchange_id="binance", symbol="BTC/USDT")
    df_raw = fetcher.fetch_ohlcv(timeframe="1h", limit=200)
    
    print(f"\nHam veri boyutu: {len(df_raw)}")
    print(df_raw.head())
    
    # Preprocessor test
    preprocessor = DataPreprocessor()
    
    # Full pipeline uygula
    df_processed = preprocessor.full_pipeline(df_raw)
    
    print(f"\nÄ°ÅŸlenmiÅŸ veri boyutu: {len(df_processed)}")
    print("\nÄ°ÅŸlenmiÅŸ veri kolonlarÄ±:")
    print(df_processed.columns.tolist())
    print("\nSon 5 satÄ±r:")
    print(df_processed.tail())
    
    print("\n" + "=" * 60)
    print("TÃœM TESTLER TAMAMLANDI")
    print("=" * 60)
